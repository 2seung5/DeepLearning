{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = '글짓기'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4574"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'C:/Users/User/Desktop/024.에세이 글 평가 데이터/01.데이터/1.Training/라벨링데이터/TL_글짓기/글짓기'  # 백슬래시를 슬래시로 변경\n",
    "\n",
    "json_list = [os.path.join(file_path, file) for file in os.listdir(file_path) if file.endswith(\".json\")]\n",
    "\n",
    "# JSON 파일 불러오기 및 확인\n",
    "data_list = []\n",
    "for json_file in json_list:\n",
    "    with open(json_file, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "        data_list.append(data)\n",
    "\n",
    "len(data_list)  # JSON 파일의 개수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/User/Desktop/024.에세이 글 평가 데이터/01.데이터/1.Training/라벨링데이터/TL_글짓기/글짓기/preprocessed_essays.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/User/Desktop/024.에세이 글 평가 데이터/01.데이터/1.Training/라벨링데이터/TL_글짓기/글짓기/preprocessed_essays.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# 데이터 불러오기\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     13\u001b[0m     essays_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# 데이터 전처리: 입력 데이터와 타겟 데이터 준비\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/User/Desktop/024.에세이 글 평가 데이터/01.데이터/1.Training/라벨링데이터/TL_글짓기/글짓기/preprocessed_essays.json'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "# 전처리된 데이터 파일 경로\n",
    "file_path = 'C:/Users/User/Desktop/024.에세이 글 평가 데이터/01.데이터/1.Training/라벨링데이터/TL_글짓기/글짓기/preprocessed_essays.json'\n",
    "\n",
    "# 데이터 불러오기\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    essays_data = json.load(file)\n",
    "\n",
    "# 데이터 전처리: 입력 데이터와 타겟 데이터 준비\n",
    "input_data = []  # 전처리된 입력 데이터\n",
    "target_data = []  # 채점 점수(라벨) 데이터\n",
    "\n",
    "# 실제 데이터셋에 따라 적절한 형태로 전처리 수행\n",
    "for essay_id, essay_content in essays_data.items():\n",
    "    # 여기에 데이터를 입력 형식에 맞게 가공하는 코드를 작성합니다.\n",
    "    # 예시로 데이터를 불러오고 더미 채점 점수(0~5)를 사용합니다.\n",
    "    input_data.append(essay_content)  # 전처리된 에세이 데이터를 입력 데이터로 사용\n",
    "    # 채점 점수는 실제 데이터에서 적절한 값으로 대체되어야 합니다.\n",
    "    target_data.append(np.random.randint(0, 6))  # 더미 채점 점수(0~5) 생성\n",
    "\n",
    "# 데이터를 numpy 배열로 변환\n",
    "input_data = np.array(input_data)\n",
    "target_data = np.array(target_data)\n",
    "\n",
    "# 훈련 데이터와 검증 데이터로 분할\n",
    "train_input, val_input, train_target, val_target = train_test_split(\n",
    "    input_data, target_data, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 데이터를 PyTorch의 Tensor로 변환\n",
    "train_input_tensor = torch.from_numpy(train_input).float()\n",
    "train_target_tensor = torch.from_numpy(train_target).long()\n",
    "val_input_tensor = torch.from_numpy(val_input).float()\n",
    "val_target_tensor = torch.from_numpy(val_target).long()\n",
    "\n",
    "# 데이터셋 및 데이터 로더 생성\n",
    "train_dataset = TensorDataset(train_input_tensor, train_target_tensor)\n",
    "val_dataset = TensorDataset(val_input_tensor, val_target_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# LSTM 모델 정의\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])  # LSTM의 마지막 타임스텝 출력만 사용\n",
    "        return out\n",
    "\n",
    "# 모델 생성 및 손실 함수, 최적화 기법 설정\n",
    "input_size = len(input_data[0])  # 입력 크기 설정 (에세이 데이터의 특성 수)\n",
    "output_size = 6  # 출력 크기 설정 (채점 점수 범위)\n",
    "hidden_size = 64  # LSTM의 hidden unit 크기 설정\n",
    "model = LSTMModel(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 모델 훈련\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # 검증 데이터셋을 사용한 모델 평가\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    for inputs, targets in val_loader:\n",
    "        outputs = model(inputs)\n",
    "        val_loss = criterion(outputs, targets)\n",
    "        val_running_loss += val_loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {running_loss/len(train_loader)} - Val Loss: {val_running_loss/len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'cp949' codec can't decode byte 0xec in position 52: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4574\u001b[39m):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(json_list[i]) \u001b[38;5;28;01mas\u001b[39;00m data_file:\n\u001b[1;32m----> 5\u001b[0m         sample_data \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m             \u001b[38;5;66;03m# dictionary 형태로 되어있는 경우\u001b[39;00m\n\u001b[0;32m      8\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m sample_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorrection\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence_corr_txt_grammar\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1776.0_x64__qbz5n2kfra8p0\\Lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(fp\u001b[38;5;241m.\u001b[39mread(),\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, object_hook\u001b[38;5;241m=\u001b[39mobject_hook,\n\u001b[0;32m    295\u001b[0m         parse_float\u001b[38;5;241m=\u001b[39mparse_float, parse_int\u001b[38;5;241m=\u001b[39mparse_int,\n\u001b[0;32m    296\u001b[0m         parse_constant\u001b[38;5;241m=\u001b[39mparse_constant, object_pairs_hook\u001b[38;5;241m=\u001b[39mobject_pairs_hook, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'cp949' codec can't decode byte 0xec in position 52: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "type_list = []\n",
    "count_correction = 0\n",
    "for i in range(4574):\n",
    "    with open(json_list[i]) as data_file:\n",
    "        sample_data = json.load(data_file)\n",
    "        try:\n",
    "            # dictionary 형태로 되어있는 경우\n",
    "            if sample_data['correction']['sentence_corr_txt_grammar'] == '':\n",
    "                pass\n",
    "            else:\n",
    "                print('t:',i)\n",
    "        except:\n",
    "            # 배열 형태로 되어있는 경우\n",
    "            #print(sample_data['correction'])\n",
    "            count_correction+=1\n",
    "            for j in range(len(sample_data['correction'])):\n",
    "                now_sample = sample_data['correction'][j]\n",
    "                \n",
    "                for k in range(len(now_sample['sentence_corr_type'])):\n",
    "                    corr_type = now_sample['sentence_corr_type'][k]\n",
    "                    corr_detail_type = now_sample['sentence_corr_detail_type'][k]\n",
    "                    corr_reason = now_sample['sentence_corr_reason'][k]\n",
    "                    type_list.append(f'{corr_type}_{corr_detail_type}_{corr_reason}')\n",
    "                    if f'{corr_type}_{corr_detail_type}_{corr_reason}' == '문법_어미_':\n",
    "                        print(now_sample)\n",
    "                        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/User/Desktop/024.에세이 글 평가 데이터/01.데이터/1.Training/라벨링데이터/TL_글짓기/글짓기\\\\글짓기_고등_1학년_ESSAY_33982.json'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_list[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
